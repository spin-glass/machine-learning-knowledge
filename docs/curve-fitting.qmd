---
title: 機械学習の基本概念の解説
format:
  html:
    code-fold: true
---

本ページでは線形回帰と多項式近似によるカーブフィッティングを例に、機械学習の基本となる以下のトピックについて解説します  

- 誤差
- 最小二乗法
- 正則化
- ハイパーパラメータ
- クロスバリデーション

# 機械学習の基礎概念

## 機械学習とは何か

機械学習とは、コンピュータがデータから学習し、予測や分類などのタスクを実行する技術です。従来のプログラムとは異なり、明確なルールを人間が記述するのではなく、データを使ってモデルを訓練し、そのモデルが新しいデータに対して適切に動作するようにします。

## 教師あり学習と教師なし学習の違い

機械学習には主に二つの学習方法があります。

- 教師あり学習: ラベル付きデータを使用してモデルを訓練します。目的変数（ターゲット）が与えられており、モデルはそのターゲットを予測するように学習します。例として、スパムメールの判定や家の価格予測があります。また、推薦システムにおいては、ユーザーの閲覧データを使ってユーザーがどのアイテムを好むかを予測します。

- 教師なし学習: ラベルなしデータを使用してモデルを訓練します。目的変数がないため、データの構造やパターンを見つけ出すことが主な目的です。例として、クラスタリングや次元削減、協調フィルタリングがあります。

今回は、機械学習の基本概念を理解することに適している教師あり学習を扱います。
以下では最も基本的な教師あり学習である回帰モデルを通じて、教師あり学習の仕組みについて説明します。

# 回帰分析

## 回帰分析の目的

回帰分析の目的は、説明変数（特徴量）と目的変数（ターゲット）の関係をモデル化し、新しいデータに対して目的変数を予測することです。回帰分析には、特に線形回帰や多項式回帰などの手法があります。

## 身長と体重による線形回帰の例

身長と体重の関係を求める線形回帰の例を見てみましょう。

```{python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
import japanize_matplotlib

# サンプルデータ生成
np.random.seed(0)
heights = np.random.normal(170, 10, 50)  # 身長: 平均170cm, 標準偏差10cm
weights = 0.5 * heights + np.random.normal(0, 5, 50)  # 体重: 身長に比例（ノイズ追加）

# 回帰分析
model = LinearRegression()
heights_reshaped = heights.reshape(-1, 1)
model.fit(heights_reshaped, weights)
predicted_weights = model.predict(heights_reshaped)

# 当てはまらない直線
incorrect_slope = -0.3
incorrect_intercept = 120
incorrect_weights = incorrect_slope * heights + incorrect_intercept

# 一番右端の点
max_height = heights.max()
max_height_index = np.argmax(heights)
actual_weight = weights[max_height_index]
predicted_weight = predicted_weights[max_height_index]
incorrect_weight = incorrect_weights[max_height_index]

# グラフ作成
plt.figure(figsize=(10, 6))
plt.scatter(heights, weights, color='blue', label='サンプルデータ')
plt.plot(heights, predicted_weights, color='red', label='回帰直線')
plt.plot(heights, incorrect_weights, color='green', label='当てはまらない直線')

# 一番右端の点の誤差を表示
plt.plot([max_height-.3, max_height-.3], [actual_weight, predicted_weight], color='red', linestyle=':', label='回帰直線との誤差')
plt.plot([max_height, max_height], [actual_weight, incorrect_weight], color='green', linestyle='--', label='当てはまらない直線との誤差')

# グラフのラベルとタイトル
plt.xlabel('身長 (cm)')
plt.ylabel('体重 (kg)')
plt.title('身長と体重の回帰分析')
plt.legend()
plt.grid(True)

# グラフ表示
plt.show()
```

この図では、1人の身長・体重が1つの点として表現されています。説明変数𝑥が身長、目的変数yが体重です。

線形回帰ではxとyに以下のような関係式が成り立つと仮定します。

<!-- \begin{align*} 
  y = ax + b
\end{align*} -->
![](./img/math/equation6.png)

上の式のように、説明変数と目的変数を数式で表現したものをモデルと言います。

線形回帰では、この式の係数aと切片b を求めることを目的とし、それにより決定されたモデルを用いて未知のデータに対して予測を行います。

上の図の赤と緑の直線は、それぞれ異なるaとbの値を持つ直線の例です。どちらの方が、よりモデルとして適しているかを確認しましょう。

今回扱っている例では、身長が増えると体重も増える傾向にあります。そのため、a>0 であることが予想され、赤い直線の方が当てはまりが良いことが感覚としてわかると思います。

以下では、この「当てはまりの良さ」を定量的に評価するために利用する誤差について説明します。

## 誤差

モデルの当てはまりの良さを定量的に調べるために誤差を利用します。

今回は、誤差としてよく利用される平均二乗誤差（Mean Squared Error, MSE）を用います。
平均二乗誤差は以下の式によって表されます。

<!-- \begin{align*}
E & = \frac{1}{2} \sum_i^n ( y_i - ( ax_i + b ) )^2
\end{align*} -->
![](./img/math/equation7.png)

平均二乗誤差以外にも、以下のような平均絶対誤差（Mean Absolute Error, MAE）も存在します。

<!-- \begin{align*}
E & = \frac{1}{n} \sum_i^n | y_i - ( ax_i + b ) |
\end{align*} -->
![](./img/math/equation8.png)

これらの誤差は、上の図の点線で示されるようなモデルの予測値と実際のデータ点との距離を、全てのデータについて合計したものになります。

この誤差が小さいモデルが、実際のデータに対して当てはまりが良いモデルであると解釈できます。

例として、右端の1つの点についての誤差を点線で示しています。赤の直線の方が緑の直線よりも誤差が小さいことがわかると思います。

## 最小二乗法

誤差を最小にするようなaとbを求める手法を最小二乗法と言います。

今回の例では、赤の直線のa,bに対し最小二乗法で求めた値を利用しています。

最後に、aとbの値はscikit-learnライブラリのLenearRegression関数で求めることができます。
scikit-learn は、Python で利用できる機械学習ライブラリで、回帰や分類、クラスタリングなど、さまざまな機械学習アルゴリズムを簡単に実装するためのツールです。  
以下に具体的な値を示しておきます。aが正の値になっています。

```{python}
print(f"回帰直線の係数 a: {model.coef_[0]:.2f}, 切片 b: {model.intercept_:.2f}")
```

# 多項式近似

次に、説明変数と目的変数の関係が直線で表せないような、より複雑な例を見てみます。

ここでは、カーブフィッティングの一環として、多項式近似を行います。カーブフィッティングとは、データに最も適合する関数を見つける手法のことです。今回のモデルの係数を決定するために、最小二乗法を用います。

主な目標は、与えられたデータからどのような関数関係が存在するかを推測することです。

データはsin関数から生成していますが、そのことは知らないものとして、説明変数xと目的変数t の関係を多項式として仮定します。

今回は、説明変数xと目的変数tの間に多項式の関係があると仮定します。

<!-- \begin{align*} 
  f(x) & = \omega_0 + \omega_1x + \omega_2x^2 + \cdots + \omega_Mx^M \\
       & = \sum_{m=0}^{M}w_mx^m
\end{align*} -->
![](./img/math/equation1.png)

Mは多項式の次数で、モデルの複雑さに関係します

w_1までを用いると、先ほどの回帰分析で用いたモデルと等しくなります。

wの値としてどのような値に決めると良いかは後ほど議論します。

## 誤差

先ほど回帰分析の例で見ました通り、下図のようにモデルにより計算される値と実際に観測された値の差を表す量を誤差と言います。

![](img/Figure_5.png)

今回は、多項式で計算される値$f(x)$と実際に観測された値$t_n$の差の二乗を全てのデータについて和をとったものを誤差として利用します。

このような誤差を平均二乗誤差（Mean Squared Error）と言い、以下の式で表されます。

<!-- \begin{align*}
E & = \frac{1}{2} \left( (f(x_1) - t_1)^2 + (f(x_2) - t_2)^2 + \cdots + (f(x_{10}) - t_{10})^2 \right) \\
  & = \frac{1}{2} \sum_{n=1}^{N}(f(x_n) - t_n)^2 \\
  & = \frac{1}{2} \sum_{n=1}^{N}(\sum_{m=0}^{M}w_mx^m - t_n)^2 
\end{align*} -->

![](./img/math/equation2.png)

今回も誤差を最小にするような多項式の係数$w$を求めることで、最適なモデルを求めます。

## 最小二乗法

上で述べた、誤差を最小化する係数を求める方法を誤差が二乗の形式で表されていることから、最小二乗法と言います

具体的には、以下の偏微分方程式を解きます。

<!-- \begin{align*}
\frac{\partial E}{\partial w_m} = 0 \quad (m = 0, \cdots , M)
\end{align*} -->

![](./img/math/equation3.png)

今回は、この式を解くことはせずに上式を解いた結果、多項式を最小にする係数wが得られているものとして話を進めます。

# モデルの複雑さ

以下では、多項式を定義する際に出てきた、次数Mとモデルの複雑さの関係について説明します。

## 多項式の次数

多項式の次数Mは、モデル選択に関連しています

次数Mがモデルの性能にどのように影響するかを理解するために、次数 M = 0, 3, 9の場合について比較をします

- 次数の低い多項式
  - M=0（定数とM=1（1次式）の多項式は、データに対して適合性が低く、sin関数をうまく表現できません。
  - ![](img/Figure_6_a.png)
  - ![](img/Figure_6_b.png)

- 中程度の次数の多項式
  - M=3 の多項式は、示された例で sin関数に最も適合しており、データと良好なバランスを示しています。
  - ![](img/Figure_6_c.png)

- 高次の多項式
  - M=9 とすると、トレーニングデータに対しては非常によくフィットしますが、過適合（オーバーフィッティング）の問題が生じます。この多項式は訓練データの各点を正確に通過しますが、データ点間で関数が大きく振動します。
  - ![](img/Figure_6_d.png)

## モデルの次数と係数の関係

モデルの次数と計数の関係は以下のようになります。

| 係数 | M = 0  |     M = 1     |        M = 3         |        M = 9         |
|-------------|--------|---------------|----------------------|----------------------|
| $w_0$   | 0.11   | 0.90          | 0.12                 | 0.26                 |
| $w_1$   |        | -1.58         | 11.20                | -66.13               |
| $w_2$   |        |               | -33.67               | 1,665.69             |
| $w_3$   |        |               | 22.43                | -15,566.61           |
| $w_4$   |        |               |                      | 76,321.23            |
| $w_5$   |        |               |                      | -217,389.15          |
| $w_6$   |        |               |                      | 370,626.48           |
| $w_7$   |        |               |                      | -372,051.47          |
| $w_8$   |        |               |                      | 202,540.70           |
| $w_9$   |        |               |                      | -46,080.94           |

M=9の場合、多項式の係数が大きくなることがわかります。これは、モデルがトレーニングデータに対して過度にフィットしている、すなわち過適合（オーバーフィッティング）していることを意味します。過適合したモデルは、トレーニングデータには非常によく適合しますが、新しいデータに対しては予測性能が低下し、汎化能力が低くなります。したがって、モデルの複雑さを適切に選ぶことが重要です。

## トレーニングデータとテストデータ

次数M=9の場合、多項式はトレーニングデータに対してはよくフィットしますが、未知のデータに対してフィットしないことがよくあります。これは過適合（オーバーフィッティング）と呼ばれます。

このことを確認するために、トレーニングデータとテストデータを分割する方法が用いられます。事前に得られたデータをトレーニングデータとテストデータに分け、学習はトレーニングデータのみを利用して行い、未知データへの当てはまりはテストデータを用いて確認します。トレーニングデータでモデルを学習し、テストデータでその汎化性能（新しいデータに対する適合度）を評価することで、過適合の問題を検出することができます。

## データ数と次数の関係

以下は、次数M=9でデータが少ない場合と多い場合を比較したものです

N = 15
![](img/Figure_8_a.png)

N = 100
![](img/Figure_8_b.png)

次数が高い場合でも、データが多ければモデルは特徴量と目的変数の関係をより正確に捉えることができます。データが多いことで、モデルはより多くのパターンを学習し、未知のデータに対しても適切に予測する能力が向上します。これにより、過適合のリスクを減らすことができます。

## モデルの汎化性能の評価

汎化性能とは、トレーニングデータで利用していない未知のデータに対する予測性能のことを言います。先ほど見た通り、多項式の次数 $M$ により汎化性能が変化します。このことを確認するために、テストセットで未見データのモデル挙動を観察します。

- 性能評価方法
テストセット上で $E(w^*)$ を計算します  
誤差$E(w^*)$ はテストデータ適合度を示し、小さい$E(w^*)$は高い汎化性能を意味します

- RMSE（Root Mean Squared Error）
誤差として、以下の式で表されるRMSEを用います。RMSEは、平均二乗誤差の平方根をとった値となっており、二乗した単位が元に戻るため人間にとって解釈しやすい値になります。

<!-- \begin{align*}
\text{RMSE} = \sqrt{\frac{1}{N} \sum_{n=1}^{N} (y(x_n, w^*) - t_n)^2}
\end{align*} -->

![](./img/math/equation4.png)

  - N: テストデータ数
  - $y(x_n, w^*)$: 予測値, $t_n$: 実際の目標値

RMSEが小さいほど予測精度が高いことを意味します。

- 性能評価の視覚化
  - 図1.7: $M$の異なる値でのトレーニングセットとテストセットの誤差をグラフ化
![](img/Figure_7.png)

# 正則化

「データ数と次数の関係」で、データの数により過適合が避けられることを確認しました。以下では過適合を避ける別の方法である正則化について説明します。

## 正則化とは

正則化は、モデルのパラメータ数を単に制限する代わりに、問題の複雑さに合わせてモデルの複雑さを調整する方法です。具体的には、誤差にペナルティ項を追加することで、係数の大きさを抑制し過適合を防ぎます

## 正則化の方法

正則化された誤差は次のように表されます。

<!-- \begin{align*}
E_{\text{reg}}(w) = \sum_{n=1}^{N} (y(x_n, w) - t_n)^2 + \lambda \sum_{j=0}^{M} w_j^2
\end{align*} -->

![](./img/math/equation5.png)

ここで $\lambda$ は正則化の強さを調整するパラメータです

## ハイパーパラメータ

$\lambda$やMは、最小二乗法により多項式の係数$\omega$を計算する際には固定します。

これらのパラメータの値は、テストデータの誤差を最小にするものに決定します。

このように、モデルの学習プロセス中に設定されるパラメータで、訓練データから直接学習されるのではなく、事前に設定されるものをハイパーパラメータと言います。

$\omega$はハイパーパラメータの一種で、モデルによって様々なものが存在します。

例えば、推薦モデルの１つであるTwoTowerモデルには以下のようなパラメータが存在します。

[TwoTowerモデルでのハイパーパラメータの例](https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/two-tower-train/runs/two-tower-train-run-20240512020226/parameters?hl=ja&project=hardy-tenure-240602)

## 正則化の効果

正則化を利用することで、$M = 9$のような高次多項式でも過適合が抑えられるようになります  
$\lambda$ の値を調整することで、モデルの適合度と汎化能力のバランスを取ります  
$\ln\lambda=-\infty$の場合、$\lambda=0$となり、正則化が行われないことを意味します

## 正則化パラメータの影響

$\lambda$の異なる値によるフィッティングの比較

- $\ln\lambda = -18$で適切なフィッティングが得られ、過適合が抑制されています

![](img/Figure_9_a.png)

- $\ln\lambda = 0$の場合は、正則化が強すぎるためモデルの係数が小さく抑えられ、データの特徴を捉えられていません

![](img/Figure_9_b.png)

- $\lambda = -\infty$ (正則化なし)の場合はフィッティングが不十分で、過適合が目立ちます

![](img/Figure_6_d.png)

## 正則化の強さと多項式の計数の関係

次数M = 9の場合の正則化パラメータと係数の関係を以下に示します  

| Coefficient | lnλ=-∞        | lnλ=-18 | lnλ=0   |
|-------------|---------------|---------|---------|
| w₀⋆         | 0.26          | 0.26    | 0.11    |
| w₁⋆         | -66.13        | 0.64    | -0.07   |
| w₂⋆         | 1,665.69      | 43.68   | -0.09   |
| w₃⋆         | -15,566.61    | -144.00 | -0.07   |
| w₄⋆         | 76,321.23     | 57.90   | -0.05   |
| w₅⋆         | -217,389.15   | 117.36  | -0.04   |
| w₆⋆         | 370,626.48    | 9.87    | -0.02   |
| w₇⋆         | -372,051.47   | -90.02  | -0.01   |
| w₈⋆         | 202,540.70    | -70.90  | -0.01   |
| w₉⋆         | -46,080.94    | 75.26   | 0.00    |

$\ln\lambda=-\infty$(正則化なし)の場合は係数の値が大きく、逆に$\ln\lambda=0$の場合は係数の値が小さいことが分かります


## 正則化による汎化性能の可視化

訓練データとテストデータにおける平均二乗根誤差（RMSエラー）を正則化の強さ$\ln\lambda$に対してプロットします。

以下の図はRMSエラーの $\ln(\lambda)$に対するグラフで、正則化の強さが汎化エラーに与える影響を示しています。

![](img/Figure_10.png)

正則化が強いほど汎化性能は上がりますが、強すぎるとデータの適合度が損なわれます。

## ハイパーパラメータの調整

これまで、テストデータセットでハイパーパラメータ $\lambda$ や $M$ の性能を確認しました
しかし、ハイパーパラメータを最適化するためには、テストデータとは別に検証データセットを用意することが一般的です
適切な $\lambda$ や $M$ の値を見つけるためには、検証セット上でのエラーが最も低いモデルを選択します
その後、テストデータを用いて最終的なモデルの評価を行います

## 検証データセットとクロスバリデーション

データをトレーニングセットと検証データセットに分け、検証セットを使用してモデルの汎化能力を評価します。  
クロスバリデーションは、データを複数のサブセットに分け、それぞれのサブセットでモデルを訓練し、残りのサブセットでテストすることで、モデルの汎化性能を確認します。

## クロスバリデーションの手法

S-foldクロスバリデーションでは、データを $S$ グループに分け、$S-1$ グループを使用して訓練し、残った1グループで性能を評価します。このプロセスを繰り返して、平均的なモデル性能を得ます。  
以下の図における赤い部分が検証データを表します。
![](img/Figure_11.png)

## ハイパーパラメータの課題

大規模なモデルやデータセットでは、ハイパーパラメータの選択肢が多く、最適な設定を見つけるのが困難です  
モデルの訓練が計算コストが高い場合、クロスバリデーションを含む多くの訓練コストが必要になることがあります

# 参考文献
- Bishop, C. M., & Bishop, H. (2024). Deep Learning: Foundations and Concepts. Springer
