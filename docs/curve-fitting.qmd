---
title: カーブフィッティングを例とした機械学習の解説
format:
  html:
    code-fold: true
---

本ページでは合成データを用いた多項式近似を例に、機械学習の基本となる以下のトピックについて解説します  

- 誤差関数
- 最小二乗法
- 正則化
- ハイパーパラメータ
- クロスバリデーション

# 機械学習の基礎概念

## 機械学習とは何か

機械学習とは、コンピュータがデータから学習し、予測や分類などのタスクを実行する技術です 
従来のプログラムとは異なり、明確なルールを人間が記述するのではなく、データを使ってモデルを訓練し、そのモデルが新しいデータに対して適切に動作するようにします

## 教師あり学習と教師なし学習の違い

機械学習には主に二つの学習方法があります。

- 教師あり学習: ラベル付きデータを使用してモデルを訓練します。目的変数（ターゲット）が与えられており、モデルはそのターゲットを予測するように学習します。例として、スパムメールの判定や家の価格予測があります。また、推薦システムにおいては、ユーザーの閲覧データを使ってユーザーがどのアイテムを好むかを予測します 

- 教師なし学習: ラベルなしデータを使用してモデルを訓練します。目的変数がないため、データの構造やパターンを見つけ出すことが主な目的です。例として、クラスタリングや次元削減、強調フィルタリングがあります

今回は、機械学習の基本概念を理解することに適している教師あり学習を扱います。

# 回帰分析

## 回帰分析の目的

回帰分析の目的は、説明変数（特徴量）と目的変数（ターゲット）の関係をモデル化し、新しいデータに対して目的変数を予測することです。特に、線形回帰や多項式回帰などの手法が存在します。

以下では、線形回帰と多項式回帰を例を通して確認します。

## 身長と体重による線形回帰の例

身長と体重の関係を求める線形回帰の例を見てみます。

```{python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
import japanize_matplotlib

# サンプルデータ生成
np.random.seed(0)
heights = np.random.normal(170, 10, 50)  # 身長: 平均170cm, 標準偏差10cm
weights = 0.5 * heights + np.random.normal(0, 5, 50)  # 体重: 身長に比例（ノイズ追加）

# 回帰分析
model = LinearRegression()
heights_reshaped = heights.reshape(-1, 1)
model.fit(heights_reshaped, weights)
predicted_weights = model.predict(heights_reshaped)

# 当てはまらない直線
incorrect_slope = -0.3
incorrect_intercept = 120
incorrect_weights = incorrect_slope * heights + incorrect_intercept

# 一番右端の点
max_height = heights.max()
max_height_index = np.argmax(heights)
actual_weight = weights[max_height_index]
predicted_weight = predicted_weights[max_height_index]
incorrect_weight = incorrect_weights[max_height_index]

# グラフ作成
plt.figure(figsize=(10, 6))
plt.scatter(heights, weights, color='blue', label='サンプルデータ')
plt.plot(heights, predicted_weights, color='red', label='回帰直線')
plt.plot(heights, incorrect_weights, color='green', label='当てはまらない直線')

# 一番右端の点の誤差を表示
plt.plot([max_height-.3, max_height-.3], [actual_weight, predicted_weight], color='red', linestyle=':', label='回帰直線との誤差')
plt.plot([max_height, max_height], [actual_weight, incorrect_weight], color='green', linestyle='--', label='当てはまらない直線との誤差')

# グラフのラベルとタイトル
plt.xlabel('身長 (cm)')
plt.ylabel('体重 (kg)')
plt.title('身長と体重の回帰分析')
plt.legend()
plt.grid(True)

# グラフ表示
plt.show()
```

図では、1人の身長・体重が１つの点として表現されています。説明変数xが身長、目的変数yが体重となります。

回帰分析ではxとyに以下のような関係式が成り立つと仮定します。

\begin{align} 
  y = ax + b
\end{align}

上の式のように、説明変数と目的変数を数式で表現したものをモデルと言います。

そして、回帰分析ではaとbを求めることを目的とし、それにより決定されたモデルを用いて未知のデータに対して予測を行います。 

上の図の赤と緑の直線は、それぞれ異なる(a, b)の値を持つ直線の例です。

どちらの方が、よりモデルとして適しているでしょうか確認しましょう。

今回扱っている例では、身長が増えると体重も増える傾向にあります。

そのため、a>0であることが予想できるため、赤い直線の方が当てはまりの良いことが感覚としてわかると思います。

以下では、この「当てはまりの良さ」を定量的に扱うための誤差について説明します。

## 誤差

直線の当てはまりの良さを定量的に調べるためには、誤差を利用します。

誤差は以下の式によって表せます。

\begin{align}
E & = \frac{1}{2} \sum_i^n ( y_i - ( ax_i + b) ) \\
\end{align}

これは、上の図の点線で表されるようなモデルと点に対する距離を、全てのデータについて足し合わせたものになります。

この誤差が小さくなるものが、実際のデータに対して当てはまりの良いモデルであると解釈できます。

例として、右端の１つの点についての誤差を点線で示しています。赤の直線の方が緑の直線よりも誤差が小さくなることがわかると思います。

## 最小二乗法

誤差を最小にするようなaとbを求める手法を最小二乗法と言います。

今回の例では、赤の直線がそれに相当します。

最後に、aとbの値はscikit-learnライブラリのLenearRegression関数で求めることができます。

以下に具体的な値を示しておきます。aが正の値になることがわかります。

```{python}
print(f"回帰直線の係数 a: {model.coef_[0]:.2f}, 切片 b: {model.intercept_:.2f}")
```

## 多項式近似

次に、説明変数と目的変数の関係が直線で表せないようなより複雑な例を見てみます。 

今回はモデルを決定する（係数を決定する）ために最小二乗法を用います。  

主な目標は、与えられたデータからどのような関数関係が存在するかを推測することです

データはsin関数から生成していますが、そのことは知らないものとしてxとtの関数として多項式を仮定します

今回は、特徴量xと目的変数tの間に多項式の関係があると仮定します

<!-- \begin{align} 
  f(x) & = \omega_0 + \omega_1x + \omega_2x^2 + \cdots + \omega_Mx^M \\
       & = \sum_{m=0}^{M}w_mx^m
\end{align} -->
![](./img/math/equation1.png)


Mは多項式の次数で、モデルの複雑さに関係します

w_1までを用いると、先ほどの回帰分析で用いたモデルと等しくなります。

wの値としてどのような値に決めると良いかは後ほど議論します

## 誤差

先ほど回帰分析の例で見ました通り、モデルにより計算される値と実際に観測された値の差を表す関数を誤差と言います

今回は、多項式で計算される値$f(x)$と実際に観測された値$t_n$の差の二乗を全てのデータについて和をとったものを誤差として利用し、特にこのような誤差を平均二乗誤差（Root Mean Squared Error）と言います。

<!-- \begin{align}
E & = \frac{1}{2} \left( (f(x_1) - t_1)^2 + (f(x_2) - t_2)^2 + \cdots + (f(x_{10}) - t_{10})^2 \right) \\
  & = \frac{1}{2} \sum_{n=1}^{N}(f(x_n) - t_n)^2 \\
  & = \frac{1}{2} \sum_{n=1}^{N}(\sum_{m=0}^{M}w_mx^m - t_n)^2 
\end{align} -->

![](./img/math/equation2.png)

![](img/Figure_5.png)

誤差関数が大きいということは、多項式が実際の観測値を表現できていないことになります

逆に、誤差関数を最小になれば、多項式は実際の観測値を推測できるということになります

そのために、誤差関数を最小にするような多項式の係数$w$を求めることが目的になります

## 最小二乗法

上で述べた、誤差を最小化する係数を求める方法を誤差関数が二乗の形式で表されていることから、最小二乗法と言います

具体的には、以下の偏微分方程式を解きます

<!-- \begin{align}
\frac{\partial E}{\partial w_m} = 0 \quad (m = 0, \cdots , M)
\end{align} -->

![](./img/math/equation3.png)

今回はこの式を解くことはせずに上式を解いた結果、多項式を最小にする係数${w}$が得られているものとして話を進めます

TensorFlow, PyTorchといったライブラリでは、自動微分という機能が搭載されており、ユーザーが関数を指定するだけで内部的に偏微分の計算を行なってくれます

# モデルの複雑さ

以下では、多項式を定義する際に出てきた、次数Mとモデルの複雑さの関係について説明します

## 多項式の次数

多項式の次数Mは、モデル選択に関連しています

次数Mがモデルの性能にどのように影響するかを理解するために、次数 M = 0, 3, 9の場合について比較をします

- 次数の低い多項式
  - M = 0（定数）と M = 1（1次式）の多項式は、データに対して適合性が低く、sin関数 をうまく表現できません
  - ![](img/Figure_6_a.png)
  - ![](img/Figure_6_b.png)

- 中程度の次数の多項式
  - M = 3 の多項式は、示された例で sin関数に最も適合しており、データと良好なバランスを示しています
  - ![](img/Figure_6_c.png)

- 高次の多項式
  - M = 9 とすると、トレーニングデータに対しては非常によくフィットしますが、過適合（オーバーフィッティング）の問題が生じます。この多項式は訓練データの各点を正確に通過しますが、データ点間で関数が大きく振動します
  - ![](img/Figure_6_d.png)

## モデルの次数と計数の関係

モデルの次数と計数の関係は以下のようになります

| 係数 | M = 0  |     M = 1     |        M = 3         |        M = 9         |
|-------------|--------|---------------|----------------------|----------------------|
| $w_0$   | 0.11   | 0.90          | 0.12                 | 0.26                 |
| $w_1$   |        | -1.58         | 11.20                | -66.13               |
| $w_2$   |        |               | -33.67               | 1,665.69             |
| $w_3$   |        |               | 22.43                | -15,566.61           |
| $w_4$   |        |               |                      | 76,321.23            |
| $w_5$   |        |               |                      | -217,389.15          |
| $w_6$   |        |               |                      | 370,626.48           |
| $w_7$   |        |               |                      | -372,051.47          |
| $w_8$   |        |               |                      | 202,540.70           |
| $w_9$   |        |               |                      | -46,080.94           |


M=9の場合、係数が大きいことがわかります

## トレーニングデータとテストデータ

次数M=9の場合、多項式はトレーニングデータに対してはよくフィットしますが、未知のデータに対してフィットしないと思われます

このことを確認するために、トレーニングデータとテストデータを分割する方法が用いられます

事前に得られたデータをトレーニングデータとテストデータに分けておき学習はトレーニングデータのみを利用して行い、未知データへの当てはまりは、テストデータを用いて確認します


## データ数と次数の関係

以下は、次数M=9でデータが少ない場合と多い場合を比較したものです

N = 15
![](img/Figure_8_a.png)

N = 100
![](img/Figure_8_b.png)

次数が高い場合でもデータが多ければ、特徴量と目的変数の関係が捉えられています

## モデルの汎化性能の評価

汎化性能とは、トレーニングデータで利用していない未知のデータに対する予測性能のことを言います  
先ほど見た通り、多項式の次数 $M$ により汎化性能が変化します  
このことを確認するために、テストセットで未見データのモデル挙動を観察します

- 性能評価方法
テストセット上で $E(w^*)$ を計算します  
誤差$E(w^*)$ はテストデータ適合度を示し、小さい$E(w^*)$は高い汎化性能を意味します

- 平均二乗根誤差（RMS Error）

<!-- \begin{align}
\text{RMS Error} = \sqrt{\frac{1}{N} \sum_{n=1}^{N} (y(x_n, w^*) - t_n)^2}
\end{align} -->

![](./img/math/equation4.png)

  - N: テストデータ数
  - $y(x_n, w^*)$: 予測値, $t_n$: 実際の目標値

RMSエラーが小さいほど予測精度が高いことを意味します

- 性能評価の視覚化
  - 図1.7: $M$の異なる値でのトレーニングセットとテストセットの誤差をグラフ化
![](img/Figure_7.png)

# 正則化

上では、データの数によりフィティングの性能が変わるということをみました  
以下では別の方法として、正則化により過適合を避ける方法を確認します

## 正則化とは

- モデルのパラメータ数を単に制限する代わりに、問題の複雑さに合わせてモデルの複雑さを調整する方法です
- 正則化は、誤差関数にペナルティ項を追加することで、係数の大きさを抑制し、過適合を防ぎます

正則化は、大きな係数に対して誤差のペナルティを課す方法で、誤差関数に対して係数の大きさに応じた項を加えることで実現します

## 正則化の方法

正則化された誤差関数は次のように表されます

<!-- \begin{align}
E_{\text{reg}}(w) = \sum_{n=1}^{N} (y(x_n, w) - t_n)^2 + \lambda \sum_{j=0}^{M} w_j^2
\end{align} -->

![](./img/math/equation5.png)

ここで $\lambda$ は正則化の強さを調整するパラメータです

## ハイパーパラメータ

$\lambda$やMは、最小二乗法により多項式の係数$\omega$を計算する際には固定します

これらのパラメータの値は、テストデータの誤差を最小にするものに決定します

このようなパラメータのことをハイパーパラメータと言い、モデルによって様々なものが存在します

[TwoTowerモデルでのハイパーパラメータの例](https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/two-tower-train/runs/two-tower-train-run-20240512020226/parameters?hl=ja&project=hardy-tenure-240602)

## 正則化の効果

正則化を利用することで、$M = 9$のような高次多項式でも過適合が抑えられるようになります  
$\lambda$ の値を調整することで、モデルの適合度と汎化能力のバランスを取ります  
$\ln\lambda=-\infty$の場合、$\lambda=0$となり、正則化が行われないことを意味します

## 正則化パラメータの影響

$\lambda$の異なる値によるフィッティングの比較

- $\ln\lambda = -18$で適切なフィッティングが得られ、過適合が抑制されています

![](img/Figure_9_a.png)

- $\ln\lambda = 0$の場合は、正則化が強すぎるためモデルの係数が小さく抑えられ、データの特徴を捉えられていません

![](img/Figure_9_b.png)

- $\lambda = -\infty$ (正則化なし)の場合はフィッティングが不十分で、過適合が目立ちます

![](img/Figure_6_d.png)

## 正則化の強さと多項式の計数の関係

次数M = 9の場合の正則化パラメータと係数の関係を以下に示します  

| Coefficient | lnλ=-∞        | lnλ=-18 | lnλ=0   |
|-------------|---------------|---------|---------|
| w₀⋆         | 0.26          | 0.26    | 0.11    |
| w₁⋆         | -66.13        | 0.64    | -0.07   |
| w₂⋆         | 1,665.69      | 43.68   | -0.09   |
| w₃⋆         | -15,566.61    | -144.00 | -0.07   |
| w₄⋆         | 76,321.23     | 57.90   | -0.05   |
| w₅⋆         | -217,389.15   | 117.36  | -0.04   |
| w₆⋆         | 370,626.48    | 9.87    | -0.02   |
| w₇⋆         | -372,051.47   | -90.02  | -0.01   |
| w₈⋆         | 202,540.70    | -70.90  | -0.01   |
| w₉⋆         | -46,080.94    | 75.26   | 0.00    |

$\ln\lambda=-\infty$(正則化なし)の場合は係数の値が大きく、逆に$\ln\lambda=0$の場合は係数の値が小さいことが分かります


## 正則化による汎化性能の可視化

訓練データとテストデータにおける平均二乗根誤差（RMSエラー）を 正則化の強さ$\ln\lambda$に対してプロット

以下の図はRMSエラーの $\ln(\lambda)$に対するグラフで、正則化の強さが汎化エラーに与える影響を示します
![](img/Figure_10.png)

正則化が強いほど汎化性能は上がりますが、強すぎるとデータの適合度が損なわれます

## ハイパーパラメータの調整

これまで、テストデータセットでハイパーパラメータ $\lambda$ や $M$ の性能を確認しました
しかし、ハイパーパラメータを最適化するためには、テストデータとは別に検証データセットを用意することが一般的です
適切な $\lambda$ や $M$ の値を見つけるためには、検証セット上でのエラーが最も低いモデルを選択します
その後、テストデータを用いて最終的なモデルの評価を行います

## 検証データセットとクロスバリデーション

データをトレーニングセットと検証データセットに分け、検証セットを使用してモデルの汎化能力を評価します  
クロスバリデーションは、データを複数のサブセットに分け、それぞれのサブセットでモデルを訓練し、残りのサブセットでテストすることで、モデルの汎化性能を確認します

## クロスバリデーションの手法

S-foldクロスバリデーションでは、データを $S$ グループに分け、$S-1$ グループを使用して訓練し、残った1グループで性能を評価します。このプロセスを繰り返して、平均的なモデル性能を得ます  
以下の図における赤い部分が検証データを表します
![](img/Figure_11.png)

## ハイパーパラメータの課題

大規模なモデルやデータセットでは、ハイパーパラメータの選択肢が多く、最適な設定を見つけるのが困難です  
モデルの訓練が計算コストが高い場合、クロスバリデーションを含む多くの訓練コストが必要になることがあります

# 参考文献
- Bishop, C. M., & Bishop, H. (2024). Deep Learning: Foundations and Concepts. Springer
